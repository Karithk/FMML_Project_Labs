{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Karithk/FMML_Project_Labs/blob/main/Module_01_Lab_02_MLPractice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Eu9VZbF01eq"
      },
      "source": [
        "# Machine learning terms and metrics\n",
        "\n",
        "FMML Module 1, Lab 2<br>\n",
        "\n",
        "\n",
        " In this lab, we will show a part of the ML pipeline by extracting features, training and testing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "metadata": {
        "id": "xvJQggGAmije",
        "outputId": "ef4733da-2212-4e75-99ad-aebc668521fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qBvyEem0vLi"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "# set randomseed\n",
        "rng = np.random.default_rng(seed=42)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3t59g5s1HfC"
      },
      "source": [
        "In this lab, we will use the California Housing dataset. There are 20640 samples, each with 8 attributes like income of the block, age of the houses per district etc. The task is to predict the cost of the houses per district.\n",
        "\n",
        "Let us download and examine the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LpqjN991GGJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e897af02-1eb4-42f8-db5b-0ff0e08fe145"
      },
      "source": [
        " dataset =  datasets.fetch_california_housing()\n",
        " # print(dataset.DESCR)  # uncomment this if you want to know more about this dataset\n",
        " # print(dataset.keys())  # if you want to know what else is there in this dataset\n",
        " dataset.target = dataset.target.astype(np.int) # so that we can classify\n",
        " print(dataset.data.shape)\n",
        " print(dataset.target.shape)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(20640, 8)\n",
            "(20640,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-60ae2e9a125e>:4: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  dataset.target = dataset.target.astype(np.int) # so that we can classify\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNx4174W5xRg"
      },
      "source": [
        "Here is a function for calculating the 1-nearest neighbours"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07zpydQj1hIQ"
      },
      "source": [
        "def NN1(traindata, trainlabel, query):\n",
        "  diff  = traindata - query  # find the difference between features. Numpy automatically takes care of the size here\n",
        "  sq = diff*diff # square the differences\n",
        "  dist = sq.sum(1) # add up the squares\n",
        "  label = trainlabel[np.argmin(dist)] # our predicted label is the label of the training data which has the least distance from the query\n",
        "  return label\n",
        "\n",
        "def NN(traindata, trainlabel, testdata):\n",
        "  # we will run nearest neighbour for each sample in the test data\n",
        "  # and collect the predicted classes in an array using list comprehension\n",
        "  predlabel = np.array([NN1(traindata, trainlabel, i) for i in testdata])\n",
        "  return predlabel"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03JktkfIGaje"
      },
      "source": [
        "We will also define a 'random classifier', which randomly allots labels to each sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fogWAtjyGhAH"
      },
      "source": [
        "def RandomClassifier(traindata, trainlabel, testdata):\n",
        "  # in reality, we don't need these arguments\n",
        "\n",
        "  classes = np.unique(trainlabel)\n",
        "  rints = rng.integers(low=0, high=len(classes), size=len(testdata))\n",
        "  predlabel = classes[rints]\n",
        "  return predlabel"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Hjf1KHs7fU5"
      },
      "source": [
        "Let us define a metric 'Accuracy' to see how good our learning algorithm is. Accuracy is the ratio of the number of correctly classified samples to the total number of samples. The higher the accuracy, the better the algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouuCqWU07bz-"
      },
      "source": [
        "def Accuracy(gtlabel, predlabel):\n",
        "  assert len(gtlabel)==len(predlabel), \"Length of the groundtruth labels and predicted labels should be the same\"\n",
        "  correct = (gtlabel==predlabel).sum() # count the number of times the groundtruth label is equal to the predicted label.\n",
        "  return correct/len(gtlabel)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vJFwBFa9Klw"
      },
      "source": [
        "Let us make a function to split the dataset with the desired probability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ko0VzpSM2Tdi"
      },
      "source": [
        "def split(data, label, percent):\n",
        "  # generate a random number for each sample\n",
        "  rnd = rng.random(len(label))\n",
        "  split1 = rnd<percent\n",
        "  split2 = rnd>=percent\n",
        "  split1data = data[split1,:]\n",
        "  split1label = label[split1]\n",
        "  split2data = data[split2,:]\n",
        "  split2label = label[split2]\n",
        "  return split1data, split1label, split2data, split2label"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcK3LEAJ_LGC"
      },
      "source": [
        "We will reserve 20% of our dataset as the test set. We will not change this portion throughout our experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBZkHBLJ1iU-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b839db27-aa88-4a28-e1da-e306c8173705"
      },
      "source": [
        "testdata, testlabel, alltraindata, alltrainlabel = split(dataset.data, dataset.target, 20/100)\n",
        "print('Number of test samples = ', len(testlabel))\n",
        "print('Number of other samples = ', len(alltrainlabel))\n",
        "print('Percent of test data = ', len(testlabel)*100/len(dataset.target),'%')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of test samples =  4144\n",
            "Number of other samples =  16496\n",
            "Percent of test data =  20.07751937984496 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6Ss0Z6IAGNV"
      },
      "source": [
        "## Experiments with splits\n",
        "\n",
        "Let us reserve some of our train data as a validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFew2iry_7W7"
      },
      "source": [
        "traindata, trainlabel, valdata, vallabel = split(alltraindata, alltrainlabel, 75/100)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60hiu4clFN1i"
      },
      "source": [
        "What is the accuracy of our classifiers on the train dataset?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBlZDTHUFTZx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "42d545b7-e18f-4e00-cb6d-7f053febb377"
      },
      "source": [
        "trainpred = NN(traindata, trainlabel, traindata)\n",
        "trainAccuracy = Accuracy(trainlabel, trainpred)\n",
        "print(\"Train accuracy using nearest neighbour is \", trainAccuracy)\n",
        "\n",
        "trainpred = RandomClassifier(traindata, trainlabel, traindata)\n",
        "trainAccuracy = Accuracy(trainlabel, trainpred)\n",
        "print(\"Train accuracy using random classifier is \", trainAccuracy)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-9be783bfadcf>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraindata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraindata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrainAccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Train accuracy using nearest neighbour is \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainAccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrainpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraindata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraindata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'traindata' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7h08-9gJDtSy"
      },
      "source": [
        "For nearest neighbour, the train accuracy is always 1. The accuracy of the random classifier is close to 1/(number of classes) which is 0.1666 in our case.\n",
        "\n",
        "Let us predict the labels for our validation set and get the accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4h7bXoW_2H3v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0560ee4-7c7e-4a86-a475-5d1929ab0625"
      },
      "source": [
        "valpred = NN(traindata, trainlabel, valdata)\n",
        "valAccuracy = Accuracy(vallabel, valpred)\n",
        "print(\"Validation accuracy using nearest neighbour is \", valAccuracy)\n",
        "\n",
        "valpred = RandomClassifier(traindata, trainlabel, valdata)\n",
        "valAccuracy = Accuracy(vallabel, valpred)\n",
        "print(\"Validation accuracy using random classifier is \", valAccuracy)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy using nearest neighbour is  0.34108527131782945\n",
            "Validation accuracy using random classifier is  0.1690891472868217\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "py9bLguFEjfg"
      },
      "source": [
        "Validation accuracy of nearest neighbour is considerably less than its train accuracy while the validation accuracy of random classifier is the same. However, the validation accuracy of nearest neighbour is twice that of the random classifier.\n",
        "\n",
        "Now let us try another random split and check the validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujm3cyYzEntE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "157bbaed-6e34-4a3b-cf8e-74ebe1d04443"
      },
      "source": [
        "traindata, trainlabel, valdata, vallabel = split(alltraindata, alltrainlabel, 75/100)\n",
        "valpred = NN(traindata, trainlabel, valdata)\n",
        "valAccuracy = Accuracy(vallabel, valpred)\n",
        "print(\"Validation accuracy of nearest neighbour is \", valAccuracy)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy of nearest neighbour is  0.34251584592881523\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSOx7U83EKie"
      },
      "source": [
        "You can run the above cell multiple times to try with different random splits.\n",
        "We notice that the accuracy is different for each run, but close together.\n",
        "\n",
        "Now let us compare it with the accuracy we get on the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNEZ5ToYBEDW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9ecfa53-1393-489e-acad-745ed370110f"
      },
      "source": [
        "testpred = NN(alltraindata, alltrainlabel, testdata)\n",
        "testAccuracy = Accuracy(testlabel, testpred)\n",
        "print('Test accuracy is ', testAccuracy)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy is  0.34917953667953666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3dGD531K3gH"
      },
      "source": [
        "### Try it out for yourself and answer:\n",
        "1. How is the accuracy of the validation set affected if we increase the percentage of validation set? What happens when we reduce it?\n",
        "2. How does the size of the train and validation set affect how well we can predict the accuracy on the test set using the validation set?\n",
        "3. What do you think is a good percentage to reserve for the validation set so that thest two factors are balanced?\n",
        "\n",
        "Answer for both nearest neighbour and random classifier. You can note down the values for your experiments and plot a graph using  <a href=https://matplotlib.org/stable/gallery/lines_bars_and_markers/step_demo.html#sphx-glr-gallery-lines-bars-and-markers-step-demo-py>plt.plot<href>. Check also for extreme values for splits, like 99.9% or 0.1%"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Answer for first question\n",
        "1. Increasing the Percentage of Validation Set:\n",
        "Pros:\n",
        "\n",
        "Better Generalization: A larger validation set provides a better estimate of the model's performance on unseen data. It helps in assessing the model's ability to generalize to new, unseen examples.\n",
        "Reduced Overfitting: With more data in the validation set, the model is less likely to overfit the validation data, resulting in a more accurate evaluation of its true performance.\n",
        "Cons:\n",
        "\n",
        "Reduced Training Data: Increasing the validation set size means reducing the amount of data available for training the model. This reduction in training data can lead to the model not learning as effectively, especially if the original dataset is small.\n",
        "Effect on Validation Accuracy:\n",
        "\n",
        "Initially, as the validation set size increases, the accuracy on the validation set might increase due to better generalization and reduced overfitting.\n",
        "However, if the validation set becomes too large, the model may not have enough training data to learn from, leading to a decrease in overall accuracy.\n",
        "2. Reducing the Percentage of Validation Set:\n",
        "Pros:\n",
        "\n",
        "More Training Data: A smaller validation set means more data available for training the model. With more training data, the model might learn better representations and perform well on unseen data if it can generalize well from the training set.\n",
        "Cons:\n",
        "\n",
        "Overfitting Risk: A small validation set might not effectively capture the model's ability to generalize. The model could overfit the validation set, leading to an overly optimistic estimate of its performance on new data.\n",
        "Effect on Validation Accuracy:\n",
        "\n",
        "Initially, as the validation set size decreases, the model might appear to have higher accuracy on the validation set due to overfitting. It's learning specific patterns in the validation set that may not generalize well.\n",
        "However, this seemingly high accuracy might not translate to good performance on truly unseen data, and the model could perform poorly in real-world applications.\n",
        "Conclusion:\n",
        "\n",
        "Finding the right balance is crucial. The size of the validation set should be large enough to provide a reliable estimate of the model's performance but not so large that it significantly reduces the training data. Cross-validation techniques, like k-fold cross-validation, can also be employed to mitigate the impact of dataset partitioning on the model's performance estimation."
      ],
      "metadata": {
        "id": "ofH1M43Hjpa_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.Answer for second question\n",
        "Larger Validation Set:\n",
        "\n",
        "When you have a relatively larger validation set, it provides a more stable and reliable estimate of the model's performance during training. This means that the validation accuracy is likely to be a better reflection of how the model will perform on unseen data.\n",
        "Smaller Validation Set:\n",
        "\n",
        "A smaller validation set might introduce more variability in the estimated performance metrics. The model's accuracy on the validation set can be sensitive to the specific data points in the validation set. As a result, it might not provide as accurate a prediction of test set performance.\n",
        "Trade-off with Training Set Size:\n",
        "\n",
        "Increasing the size of the validation set usually means reducing the size of the training set. This trade-off can impact the model's ability to learn effectively. If the training set is too small, the model may not generalize well, and this can lead to overly optimistic estimates of validation performance.\n",
        "Overfitting and Underfitting:\n",
        "\n",
        "If the validation set is too small, there is a risk that the model might overfit the validation data. Overfitting means the model has learned to perform well on this specific validation set, but it may not generalize to new data. As a result, predictions about test set accuracy could be overly optimistic.\n",
        "Conversely, if the validation set is large, it is less likely to be overfit. However, the model's performance could still be negatively affected if the training set becomes too small.\n",
        "Cross-Validation:\n",
        "\n",
        "In many cases, practitioners use cross-validation techniques (e.g., k-fold cross-validation) to mitigate the impact of dataset partitioning on the prediction of test set accuracy. Cross-validation involves splitting the dataset into multiple subsets, and each subset is used as both a validation set and part of the training set in different iterations. This provides a more robust estimate of model performance.\n",
        "In summary, the size of the training and validation sets can affect the reliability of using the validation set to predict the accuracy on the test set. It's essential to strike a balance between the size of the validation set and the training set, and consider cross-validation techniques to obtain a more accurate estimate of how the model will perform on unseen data."
      ],
      "metadata": {
        "id": "Rszrikn8kYzz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.Answer for third question\n",
        "The choice of the percentage to reserve for the validation set is often a balance between having a sufficiently large dataset for training the model effectively and having a representative subset to estimate the model's performance reliably. There isn't a one-size-fits-all answer to what percentage is best, as it depends on the specific problem, the size of your dataset, and the nature of the data.\n",
        "\n",
        "However, a common practice is to use a 70-30 or 80-20 split for the training and validation sets respectively. That is, you might use 70% or 80% of your data for training the model and reserve the remaining 30% or 20% for validation. This split strikes a balance between having a substantial amount of data for training the model and a reasonably sized validation set for performance evaluation.\n",
        "\n",
        "For very large datasets, an 80-10-10 split (80% training, 10% validation, 10% test) might be appropriate. This allows for a large training set while still maintaining sizable validation and test sets.\n",
        "\n",
        "Additionally, if you're dealing with a relatively small dataset, you might consider techniques like k-fold cross-validation. In k-fold cross-validation, the data is divided into k subsets, and the model is trained and validated k times, each time using a different subset as the validation set and the remaining data as the training set. This approach provides a robust way to estimate the model's performance, especially when you have limited data.\n",
        "\n",
        "In summary, there's no fixed percentage that works for all situations. It's crucial to consider the size of your dataset, the complexity of your model, and the goals of your analysis. Experimentation and, if applicable, cross-validation can help you find the optimal split ratio for your specific machine learning problem."
      ],
      "metadata": {
        "id": "dG0FC6BGk5Mj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2fJvQ69ckgxK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "YFJncXIBjyVA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnYvkAZLQY7h"
      },
      "source": [
        "## Multiple Splits\n",
        "\n",
        "One way to get more accurate estimates for the test accuracy is by using <b>crossvalidation</b>. Here, we will try a simple version, where we do multiple train/val splits and take the average of validation accuracies as the test accuracy estimation. Here is a function for doing this. Note that this function will take a long time to execute."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4nGCUQXBTzo"
      },
      "source": [
        "# you can use this function for random classifier also\n",
        "def AverageAccuracy(alldata, alllabel, splitpercent, iterations, classifier=NN):\n",
        "  accuracy = 0\n",
        "  for ii in range(iterations):\n",
        "    traindata, trainlabel, valdata, vallabel = split(alldata, alllabel, splitpercent)\n",
        "    valpred = classifier(traindata, trainlabel, valdata)\n",
        "    accuracy += Accuracy(vallabel, valpred)\n",
        "  return accuracy/iterations # average of all accuracies"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3qtNar7Bbik",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7f06259-2fb6-4f8b-b646-682726cdd48e"
      },
      "source": [
        "print('Average validation accuracy is ', AverageAccuracy(alltraindata, alltrainlabel, 75/100, 10, classifier=NN))\n",
        "testpred = NN(alltraindata, alltrainlabel, testdata)\n",
        "print('test accuracy is ',Accuracy(testlabel, testpred) )"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average validation accuracy is  0.3393465110188342\n",
            "test accuracy is  0.34917953667953666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33GIn4x5VH-d"
      },
      "source": [
        "This is a very simple way of doing cross-validation. There are many well-known algorithms for cross-validation, like k-fold cross-validation, leave-one-out etc. This will be covered in detail in a later module. For more information about cross-validation, check <a href=https://en.wikipedia.org/wiki/Cross-validation_(statistics)>Cross-validatioin (Wikipedia)</a>\n",
        "\n",
        "### Questions\n",
        "1. Does averaging the validation accuracy across multiple splits give more consistent results?\n",
        "2. Does it give more accurate estimate of test accuracy?\n",
        "3. What is the effect of the number of iterations on the estimate? Do we get a better estimate with higher iterations?\n",
        "4. Consider the results you got for the previous questions. Can we deal with a very small train dataset or validation dataset by increasing the iterations?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Answer for first question\n",
        "Reduces Variability: Since different subsets of data are used for validation in each fold, cross-validation provides a way to reduce the impact of the particular data points in the validation set. This reduction in variability leads to a more stable estimate of the model's performance.\n",
        "\n",
        "Better Generalization: By training and validating the model on different subsets of data, cross-validation provides a better indication of how well the model generalizes to unseen data. It ensures that the evaluation is not biased by a specific random split of the data.\n",
        "\n",
        "Increased Confidence: Averaging the results over multiple folds increases your confidence in the model's performance metrics. It gives you a more accurate estimate of how the model is likely to perform in real-world scenarios.\n",
        "\n",
        "Effective Use of Data: Cross-validation allows you to make efficient use of your limited dataset. Instead of having a separate validation set that reduces the amount of data available for training, cross-validation uses all available data for both training and validation, albeit in different iterations.\n",
        "\n",
        "Popular types of cross-validation methods include k-fold cross-validation and stratified k-fold cross-validation. In k-fold cross-validation, the dataset is divided into k subsets, and the model is trained and validated k times, with each of the k subsets used once as the validation data. Stratified k-fold cross-validation ensures that each fold preserves the percentage of samples for each class, which is particularly useful for imbalanced datasets.\n",
        "\n",
        "In summary, cross-validation is a valuable technique for assessing and selecting models, especially when you have a limited dataset. It provides more consistent and reliable estimates of a model's performance by averaging the results over multiple splits, leading to better-informed decisions in the model development process."
      ],
      "metadata": {
        "id": "lWvkItWwlVO1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "s0Sdk8NVldDi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.Answer for seccond question\n",
        "Reduced Variance: Cross-validation helps in reducing the variance of the model evaluation process. Since different subsets of the data are used for training and validation in each fold, the variability introduced by a single split is mitigated. Averaging over multiple folds smoothens out the evaluation process, providing a more stable and accurate estimate of the model's performance.\n",
        "\n",
        "Better Generalization: By training the model on different subsets of data and evaluating it on different validation sets, cross-validation offers a better indication of how well the model generalizes to unseen data. It provides a more comprehensive view of the model's ability to handle different parts of the dataset.\n",
        "\n",
        "Effective Use of Data: Cross-validation allows you to use the entire dataset for both training and validation, albeit in different iterations. This is particularly valuable when you have a limited amount of data. It ensures that every data point is used for validation exactly once, maximizing the information extracted from the dataset.\n",
        "\n",
        "Model Selection: Cross-validation is often used not just for estimating the model's performance but also for model selection, hyperparameter tuning, and feature selection. By evaluating different models and configurations across multiple folds, you can choose the best-performing model, leading to a more accurate estimate of test accuracy for the selected model.\n",
        "\n",
        "Identifying Overfitting: Cross-validation can help in identifying whether a model is overfitting. If a model performs well on the training data but poorly on the cross-validated sets, it indicates overfitting. This insight can guide you in selecting a model that is less likely to overfit new, unseen data.\n",
        "\n",
        "In summary, while cross-validation does not directly give the test accuracy on your final, unseen test set, it provides a more accurate and reliable estimate of how well your model is likely to perform on new, unseen data. This estimate is based on a thorough evaluation process that incorporates multiple train-validation splits, making cross-validation a valuable technique in the model evaluation and selection process.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GL1cLWBKlvil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.Answer for third question\n",
        "Low Number of Iterations (K-Fold Cross-Validation):\n",
        "\n",
        "Pros: Fewer iterations mean faster computation, especially for large datasets or complex models. It provides a quick estimate of the model's performance.\n",
        "Cons: The estimate might be more sensitive to the specific random split of the data. A single random split might not be representative of the dataset's variability, leading to a less reliable estimate.\n",
        "Moderate Number of Iterations:\n",
        "\n",
        "Pros: A moderate number of iterations, say 5 to 10 folds, balances the trade-off between computational efficiency and reliable estimation. It provides a reasonably stable estimate of the model's performance.\n",
        "Cons: If the dataset is small, there might be limited data in each fold for training, which can affect the model's ability to learn effectively.\n",
        "High Number of Iterations (Leave-One-Out Cross-Validation or Stratified K-Fold with Many Folds):\n",
        "\n",
        "Pros: Using a large number of iterations, such as leave-one-out cross-validation (LOOCV) or a very high number of folds (e.g., 20 or more), provides a highly stable and reliable estimate. It significantly reduces the impact of the specific random splits and provides a thorough evaluation of the model's generalization.\n",
        "Cons: It can be computationally expensive, especially for large datasets, as the model needs to be trained and evaluated multiple times.\n",
        "Better Estimate with Higher Iterations:\n",
        "More Stable Estimate: With higher iterations, the estimate of the model's performance becomes more stable and less sensitive to the randomness in the data splits. Averaging over a larger number of folds provides a smoother and more reliable evaluation metric.\n",
        "\n",
        "Robustness to Data Variability: Models trained and evaluated across multiple iterations are better able to handle the variability in the dataset. This is particularly important when the dataset is small or when there are variations in the data that could affect the model's performance.\n",
        "\n",
        "Better Decision Making: When you're comparing multiple models, algorithms, or hyperparameter configurations, a more stable estimate helps in making informed decisions. It ensures that the model selection process is based on a robust evaluation of each option's performance.\n",
        "\n",
        "In summary, using a higher number of iterations in cross-validation generally leads to a better estimate of the model's performance. However, the choice of the number of iterations should consider the computational resources available and strike a balance between accuracy and efficiency. It's essential to choose a number of iterations that provides a stable and reliable estimate without unnecessary computational burden.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cpP06t_OmGT4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.Answer for fourth question\n",
        "Limited Data: If the training dataset is extremely small, there might be insufficient data in each fold for the model to learn effectively. No amount of iteration can compensate for a severe lack of representative training data. It's crucial to have a minimum amount of data for the model to capture meaningful patterns.\n",
        "\n",
        "Risk of Overfitting to Validation Set: If the validation dataset is very small, there's a risk that the model might overfit to this specific set. Averaging over more folds can reduce this risk, but it's essential to ensure that the validation set is representative and diverse enough to reflect the real-world variability of the data.\n",
        "\n",
        "Computational Cost: While increasing the number of iterations can improve stability, it comes at the cost of increased computational requirements, especially for resource-intensive models or large datasets. This needs to be balanced against the benefits gained.\n",
        "\n",
        "Best Practices:\n",
        "Stratification: If the dataset is small and potentially imbalanced, using stratified sampling in cross-validation ensures that each fold maintains the same class distribution as the whole dataset. This can lead to more representative splits.\n",
        "\n",
        "Consider Other Techniques: If the dataset is extremely small, techniques such as bootstrapping or data augmentation might be explored to artificially increase the size of the dataset. However, these techniques should be applied with caution, ensuring that they do not introduce biases into the data.\n",
        "\n",
        "In summary, while increasing the number of iterations in cross-validation can help deal with small training or validation datasets to some extent, it's important to recognize the limitations and ensure that the dataset used for validation is representative of the overall data distribution. Additionally, exploring alternative techniques and understanding the trade-offs involved is crucial when dealing with very limited data.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "d9aQctzWmbTh"
      }
    }
  ]
}